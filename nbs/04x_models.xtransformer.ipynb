{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp xtransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.basics import *\n",
    "from transformers_sandbox.core import *\n",
    "from transformers_sandbox.layers import *\n",
    "from transformers_sandbox.attention import *\n",
    "from transformers_sandbox.transformer import LMMixin, EncDecMixin\n",
    "from transformers_sandbox.config import ConfigBase, update_sig\n",
    "from transformers_sandbox.experimental import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XTransformer\n",
    "\n",
    "> Generic model for testing new building blocks. WIP..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def wrap_sublayer(sublayer:Module, method:str, d_model):\n",
    "    \"\"\"\n",
    "    Wraps a sublayer with skip connection defined by method\n",
    "    currently supported:\n",
    "    * postnorm\n",
    "    * prenorm\n",
    "    * admin\n",
    "    * rezero\n",
    "    * ...\n",
    "    \"\"\"\n",
    "    if method == 'postnorm':\n",
    "        return PostNorm(d_model, Residual(sublayer))\n",
    "    elif method == 'prenorm':\n",
    "        return Residual(PreNorm(d_model, sublayer))\n",
    "    elif method == 'admin':\n",
    "        raise NotImplementedError\n",
    "    elif method == 'rezero':\n",
    "        return ReZero(sublayer)\n",
    "    else:\n",
    "        raise NotImplementedError(f'{method} is not valid wrapping method.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bricks\n",
    "\n",
    "> Architecture specific layers, blocks and containers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class XEncoderBlock(Module):\n",
    "    \"\"\"\n",
    "    Experimental encoder block\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_model:int, \n",
    "                 n_heads:int = 8,\n",
    "                 attn_module:Module = Attention,\n",
    "                 ff_module:Module = FeedForward,\n",
    "                 d_ff:int = None, \n",
    "                 attn_dropout:float = 0.1,\n",
    "                 ff_dropout:float = 0.1,\n",
    "                 causal:bool = False, \n",
    "                 attn_bias:bool = False, \n",
    "                 residual_type:str = 'postnorm',\n",
    "                 shared_qk:bool=False,\n",
    "                 **kwargs):\n",
    "        store_attr('attn_dropout') # mb separate argument attn_post_dropout\n",
    "        attn = attn_module(d_model, n_heads=n_heads, causal=causal, dropout=attn_dropout, bias=attn_bias, shared_qk=shared_qk)\n",
    "        ff = ff_module(d_model, d_ff=d_ff, dropout=ff_dropout)\n",
    "        self.attn = wrap_sublayer(attn, method=residual_type, d_model=d_model)\n",
    "        self.ff = wrap_sublayer(ff, method=residual_type, d_model=d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None): #? more args\n",
    "        out = self.attn(x, mask=mask)\n",
    "        return self.ff(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "m = XEncoderBlock(d)\n",
    "out = m(x)\n",
    "assert (out.size() == (bs, sl, d))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "m = XEncoderBlock(d, shared_qk=True)\n",
    "out = m(x)\n",
    "assert (out.size() == (bs, sl, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class XEncoder(Module):\n",
    "    \"\"\"Stack of XEncoderBlocks\"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 n_layers=6, \n",
    "                 n_heads=8, \n",
    "                 d_ff=None,\n",
    "                 attn_module:Module=Attention,\n",
    "                 ff_module:Module=FeedForward,\n",
    "                 ff_dropout=0.1, \n",
    "                 attn_dropout=0.1,\n",
    "                 attn_bias=False,\n",
    "                 causal=False, \n",
    "                 residual_type:str='postnorm',\n",
    "                 shared_qk:bool=False,\n",
    "                 final_norm=None,\n",
    "                 **kwargs):\n",
    "        store_attr('d_model')\n",
    "        self.layers = nn.ModuleList([])    \n",
    "        for _ in range(n_layers):\n",
    "            self.layers.append(XEncoderBlock(d_model, n_heads, causal=causal, attn_module=attn_module,\n",
    "                                    d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout, ff_module=ff_module,\n",
    "                                    residual_type=residual_type, attn_bias=attn_bias, shared_qk=shared_qk,\n",
    "                                            **kwargs))\n",
    "        self.norm = None if final_norm is None else final_norm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers: x = layer(x, mask=mask)\n",
    "        if self.norm is not None: x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# postnorm\n",
    "x = torch.randn(bs, sl, d)\n",
    "m = XEncoder(d)\n",
    "out = m(x)\n",
    "assert (out.size() == (bs, sl, d))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rezero: all residual branches are zeroed out at initialization\n",
    "m = XEncoder(d, residual_type='rezero')\n",
    "out  = m(x)\n",
    "assert (out.size() == (bs, sl, d))\n",
    "assert (out == x).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class XDecoderBlock(Module):\n",
    "    \"\"\"\n",
    "    Standart transformer decoder block. Consist of self-attention, encoder-decoder attention \n",
    "    and positiona feed-forward alyers\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_model, \n",
    "                 n_heads = 8, \n",
    "                 d_ff = None,\n",
    "                 attn_dropout = 0.1, \n",
    "                 ff_dropout=0.1,\n",
    "                 mask = None ,\n",
    "                 attn_bias=False,\n",
    "                 residual_type='postnorm'):\n",
    "        # mb separate argument attn_post_dropout\n",
    "        attn = Attention(d_model, n_heads=n_heads, causal=True, dropout=attn_dropout, bias=attn_bias)\n",
    "        cross = Attention(d_model, n_heads=n_heads, causal=False, dropout=attn_dropout, bias=attn_bias)\n",
    "        ff = FeedForward(d_model, d_ff=d_ff, dropout=ff_dropout)\n",
    "        self.attn = wrap_sublayer(attn, method=residual_type, d_model=d_model)\n",
    "        self.cross = wrap_sublayer(cross, method=residual_type, d_model=d_model)\n",
    "        self.ff = wrap_sublayer(ff, method=residual_type, d_model=d_model)\n",
    "        \n",
    "    def forward(self, x, context, mask=None, context_mask=None):\n",
    "        out = self.attn(x, mask=mask)\n",
    "        out = self.cross(out, context, mask=mask, context_mask=context_mask)\n",
    "        return self.ff(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class XDecoderBlockV2(Module):\n",
    "    \"\"\"Transformer decoder block using additive attention layer instead of self-attention \n",
    "    followed by cross-attention\"\"\"\n",
    "    def __init__(self,\n",
    "                 d_model, \n",
    "                 n_heads = 8, \n",
    "                 mask = None, \n",
    "                 d_ff=None,\n",
    "                 attn_dropout=0.1, \n",
    "                 ff_dropout=0.1, \n",
    "                 attn_bias=False,\n",
    "                 residual_type='postnorm'):\n",
    "        attn = AdditiveAttention(d_model, n_heads=n_heads, causal=True, dropout=attn_dropout, bias=attn_bias)\n",
    "        ff = FeedForward(d_model, d_ff=d_ff, dropout=ff_dropout)\n",
    "        self.attn = wrap_sublayer(attn, method=residual_type, d_model=d_model)\n",
    "        self.ff = wrap_sublayer(ff, method=residual_type, d_model=d_model)\n",
    "        \n",
    "    def forward(self, x, context, mask=None, context_mask=None):\n",
    "        out = self.attn(x, context, mask=mask, context_mask=context_mask)\n",
    "        out = self.ff(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class XDecoder(Module):\n",
    "    \"\"\"Stack of TransformerDecoder layers\"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 n_layers=6, \n",
    "                 n_heads=8, \n",
    "                 d_ff=None, \n",
    "                 attn_dropout=0.1, \n",
    "                 ff_dropout=0.1, \n",
    "                 residual_type='postnorm', \n",
    "                 comb_attn=False, \n",
    "                 attn_bias=False, \n",
    "                 final_norm=None):\n",
    "        store_attr('d_model')\n",
    "        #TODO(Arto) refactor\n",
    "        block = XDecoderBlockV2 if comb_attn else XDecoderBlock\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(n_layers):\n",
    "            self.layers.append(block(d_model, n_heads, d_ff=d_ff, attn_dropout=attn_dropout, \n",
    "                                     ff_dropout=ff_dropout, residual_type=residual_type, attn_bias=attn_bias))\n",
    "        self.norm = None if final_norm is None else final_norm(d_model)\n",
    "        \n",
    "    def forward(self, x, context, mask=None, context_mask=None):\n",
    "        for layer in self.layers: x = layer(x, context, mask, context_mask)\n",
    "        if self.norm is not None: x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(bs, sl, d)\n",
    "context = torch.randn(bs, sl, d)\n",
    "m = XDecoder(d)\n",
    "out = m(x, context)\n",
    "assert (out.size() == (bs, sl, d))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class XTransformerLM(Module, LMMixin):\n",
    "    \"\"\"\n",
    "    Basic Transformer for language modelling\n",
    "    \n",
    "    Parameters:\n",
    "        * vocab_sz: int\n",
    "        * d_model: int - inner dimension of the model\n",
    "        * n_layers: int (default: 6) \n",
    "        * n_heads: int (default: 8)\n",
    "        * d_ff: int - inner dimension of the pointwise FeedForward net, if None defaults to 4*d_model\n",
    "        * attn_dropout: float - attention dropout\n",
    "        * ff_dropout: float - feed-forward dropout\n",
    "        * emb_dropout: float - embedding dropout\n",
    "        * causal: bool (default: True) - if True does causal masking automatically\n",
    "        * max_seq_len: int (default: 512)\n",
    "        * tie_weights: bool - if True target embedding weights are used for computation output projection\n",
    "        * residual_type: str - one of {'postnorm', 'prenorm', 'admin', 'rezero'}\n",
    "        * attn_bias: bool - wether to allow biases in attention projection layers\n",
    "        * pad_idx: int - padding token id, required for autogeneration of padding mask\n",
    "        * pos_enc: str from {'absolute', 'fixed', 'axial'} - type of positional encoding to use\n",
    "        * axial_shape: tuple - [optional] should be factors of max_seq_len\n",
    "        * axial_emb_dims: tuple - [optional] axial embedding components, should sum to d_model\n",
    "    Inputs:\n",
    "        * x - input ids, shape [bs, sl]\n",
    "        * mask - optional boolean mask, shape [bs, sl]\n",
    "    Returns:\n",
    "        * logits - target token logits, shape [bs, sl, vocab_sz]\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 vocab_sz:int, \n",
    "                 d_model:int, \n",
    "                 n_layers:int=6,\n",
    "                 n_heads:int=8,\n",
    "                 d_ff:int=None,\n",
    "                 attn_module:Module=Attention,\n",
    "                 ff_module:Module=FeedForward,\n",
    "                 attn_dropout:float=0.1,\n",
    "                 ff_dropout:float=0.1,\n",
    "                 emb_dropout:float=0.1,\n",
    "                 tie_weights:bool=True,\n",
    "                 causal:bool=True,\n",
    "                 pos_enc:str='absolute',\n",
    "                 max_seq_len:int=512,\n",
    "                 axial_shape:tuple=None,\n",
    "                 axial_emb_dims:tuple=None,\n",
    "                 pad_idx:int=None,\n",
    "                 residual_type:str='postnorm',\n",
    "                 attn_bias:bool=False,\n",
    "                 shared_qk:bool=False):\n",
    "        store_attr()\n",
    "        self.emb = TransformerEmbedding(vocab_sz, d_model, max_seq_len, dropout=emb_dropout, \n",
    "                                        pos_enc=pos_enc, axial_shape=axial_shape, \n",
    "                                        axial_emb_dims=axial_emb_dims)\n",
    "        final_norm = nn.LayerNorm if (residual_type in {'prenorm'}) else None\n",
    "        self.encoder = XEncoder(d_model, n_layers, n_heads, causal=causal, d_ff=d_ff,\n",
    "                                attn_module=attn_module, ff_module=ff_module,\n",
    "                                attn_dropout=attn_dropout, ff_dropout=ff_dropout,\n",
    "                                residual_type=residual_type, attn_bias=attn_bias,\n",
    "                                shared_qk=shared_qk, final_norm=final_norm)\n",
    "        self.proj = nn.Linear(d_model, vocab_sz)\n",
    "        if tie_weights: self.proj.weight = self.emb.emb.weight\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.emb(x)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return self.proj(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "vocab_sz = 256\n",
    "x = torch.randint(vocab_sz, (bs, sl))\n",
    "model = XTransformerLM(vocab_sz, d, n_layers=2, causal=False)\n",
    "out = model(x)\n",
    "assert (out.size() == (bs, sl, vocab_sz))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "model = XTransformerLM(vocab_sz, d, n_layers=2, causal=True, residual_type='prenorm')\n",
    "out = model(x)\n",
    "assert (out.size() == (bs, sl, vocab_sz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#add tests for various configs here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# model = XTransformerLM(vocab_sz, d, n_layers=2, causal=True, shared_qk=True)\n",
    "# out = model(x)\n",
    "# assert (out.size() == (bs, sl, vocab_sz))\n",
    "# assert isinstance(model.encoder.layers[0].attn.sublayer.sublayer.in_proj, SharedQKAttnInProj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# def xtransformer_lm_splits(model):\n",
    "#     \"Splits TransformerLM `model` into groups for differential learning rates.\"\n",
    "#     groups = L([model.emb] + [l for l in model.encoder.layers] + [model.proj])\n",
    "#     return groups.map(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# assert len(transformer_lm_splits(model)) == 2+2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Warning: Not implemented yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class XTransformer(Module, EncDecMixin):\n",
    "    \"\"\"\n",
    "    Basic Transformer Encoder-Decoder model\n",
    "    Parameters:\n",
    "        * enc_vocab_sz: int - source vocab size \n",
    "        * dec_vocab_sz: int - target vocab size\n",
    "        * d_model: int - inner dimension of the model\n",
    "        * n_enc_layers: int (default: 6) \n",
    "        * n_dec_layers: int (default: 6) \n",
    "        * heads: int (default: 8)\n",
    "        * d_ff: int - inner dimension of the pointwise FeedForward net, if None defaults to 4*d_model\n",
    "        * attn_dropout: float - attention dropout\n",
    "        * ff_dropout: float - feed-forward dropout\n",
    "        * emb_dropout: float - embedding dropout\n",
    "        * max_seq_len: int (default: 512)\n",
    "        * prenorm: bool - whether to use PreNorm or PostNorm\n",
    "        * attn_bias: bool - whether to allow biases in attention projection layers\n",
    "        * pad_idx: int - padding token id, if pad_idx is provided, and no mask/context_mask are \n",
    "                passed to forward method will be used to generate padding masks\n",
    "        * tie_weights: bool - if True target embedding weights are used for computation output projection\n",
    "        * shared_emb: bool - if True encoder and decoder will use shared embedding layer\n",
    "        * pos_enc: str from {'absolute', 'fixed', 'axial'} - type of positional encoding to use\n",
    "        * axial_shape: tuple - [optional] should be factors of max_seq_len\n",
    "        * axial_emb_dims: tuple - [optional] axial embedding components, should sum to d_model\n",
    "    Inputs:\n",
    "        * src - source input ids, shape [bs, src_sl]\n",
    "        * tgt - target input ids, shape [bs, tgt_sl]\n",
    "        * src_mask - optional boolean source mask, shape [bs, src_sl]\n",
    "        * tgt_mask - optional boolean target mask, shape [bs, tgt_sl]\n",
    "    Returns:\n",
    "        * logits - target token logits, shape [bs, tgt_sl, tgt_vocab_sz]\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 enc_vocab_sz, \n",
    "                 dec_vocab_sz, \n",
    "                 d_model, \n",
    "                 n_enc_layers=6, \n",
    "                 n_dec_layers=6, \n",
    "                 n_heads=8, \n",
    "                 d_ff=None,\n",
    "                 pad_idx=None, \n",
    "                 tie_weights=True,\n",
    "                 shared_emb = False,\n",
    "                 attn_dropout=0.1, \n",
    "                 ff_dropout=0.1, \n",
    "                 emb_dropout=0.1,\n",
    "                 prenorm=False, \n",
    "                 attn_bias=False,\n",
    "                 comb_attn=False, \n",
    "                 pos_enc='absolute', \n",
    "                 max_seq_len=512, \n",
    "                 axial_shape=None, \n",
    "                 axial_emb_dims=None):\n",
    "        store_attr()\n",
    "        self.enc_emb = TransformerEmbedding(enc_vocab_sz, d_model, max_seq_len, dropout=emb_dropout, pos_enc=pos_enc,\n",
    "                                            axial_shape=axial_shape, axial_emb_dims=axial_emb_dims)\n",
    "        if shared_emb:\n",
    "            assert (enc_vocab_sz == dec_vocab_sz), \"Encoder and decoder vocab size doesn't match\"\n",
    "            self.dec_emb = self.enc_emb\n",
    "        else:\n",
    "            self.dec_emb = TransformerEmbedding(dec_vocab_sz, d_model, max_seq_len, dropout=emb_dropout, pos_enc=pos_enc,\n",
    "                                                axial_shape=axial_shape, axial_emb_dims=axial_emb_dims)\n",
    "        final_norm = nn.LayerNorm if prenorm else None\n",
    "        self.encoder = TransformerEncoder(d_model, n_enc_layers, n_heads, d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout, \n",
    "                                          prenorm=prenorm, attn_bias=attn_bias, final_norm=final_norm, causal=False)\n",
    "        self.decoder = TransformerDecoder(d_model, n_dec_layers, n_heads, d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout, \n",
    "                                          prenorm=prenorm, comb_attn=comb_attn, attn_bias=attn_bias, final_norm=final_norm)\n",
    "        self.proj = nn.Linear(d_model, dec_vocab_sz)\n",
    "        if tie_weights: self.proj.weight = self.dec_emb.emb.weight\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        src_mask = default(src_mask, self.get_padding_mask(src))\n",
    "        tgt_mask = default(tgt_mask, self.get_padding_mask(tgt))\n",
    "        enc = self.encoder(self.enc_emb(src), mask=src_mask)\n",
    "        out = self.decoder(self.dec_emb(tgt), context=enc, mask=tgt_mask, context_mask=src_mask)\n",
    "        return self.proj(out)\n",
    "    \n",
    "    def get_padding_mask(self, x):\n",
    "        if self.pad_idx is None: return None\n",
    "        return (x != self.pad_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# bs = 4\n",
    "# src_sl = 70\n",
    "# tgt_sl = 80\n",
    "# d = 64\n",
    "# src_vocab_sz = 256\n",
    "# tgt_vocab_sz = 256\n",
    "# src = torch.randint(src_vocab_sz, (bs, src_sl))\n",
    "# tgt = torch.randint(tgt_vocab_sz, (bs, tgt_sl))\n",
    "# model = Transformer(src_vocab_sz, tgt_vocab_sz, d, n_enc_layers=2, n_dec_layers=2)\n",
    "# out = model(src, tgt)\n",
    "# assert (out.size() == (bs, tgt_sl, tgt_vocab_sz))\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#TODO find out what is the best way to split encoder-decoder architecture\n",
    "# def transformer_splits(model):\n",
    "#     \"[v0] Splits Transformer `model` into groups for differential learning rates.\"\n",
    "#     groups = L([nn.ModuleList([model.enc_emb, model.dec_emb])] + [l for l in model.encoder.layers] + [l for l in model.decoder.layers] + [model.proj])\n",
    "#     return groups.map(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# assert len(transformer_splits(model)) == 2+4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class XConfig(ConfigBase):\n",
    "    \"\"\"\n",
    "    Config for enwik8 Experiment.\n",
    "    See https://arampacha.github.io/reformer_fastai/experiment.enwik8-baseline.html for details\n",
    "    \"\"\"\n",
    "    _model = XTransformerLM\n",
    "    _d = {\n",
    "        'vocab_sz':256,\n",
    "        'd_model':512,\n",
    "        'n_layers':12,\n",
    "        'n_heads':8,\n",
    "        'attn_module':Attention,\n",
    "        'ff_module':FeedForward,\n",
    "        'd_ff':None,\n",
    "        'attn_dropout':0.1,\n",
    "        'ff_dropout':0.1,\n",
    "        'emb_dropout':0.1,\n",
    "        'tie_weights':True,\n",
    "        'causal':True,\n",
    "        'pos_enc':'absolute',\n",
    "        'max_seq_len':512,\n",
    "        'axial_shape':None,\n",
    "        'axial_emb_dims':None,\n",
    "        'pad_idx':None,\n",
    "        'attn_bias':False,\n",
    "        'shared_qk':False,\n",
    "        'residual_type':'postnorm',\n",
    "    }\n",
    "    @update_sig(_d)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XTransformerLM(\n",
       "  (emb): TransformerEmbedding(\n",
       "    (emb): Embedding(256, 512)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (pos_enc): AbsolutePositionalEmbedding(\n",
       "      (emb): Embedding(512, 512)\n",
       "    )\n",
       "  )\n",
       "  (encoder): XEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): XEncoderBlock(\n",
       "        (attn): ReZero(\n",
       "          (sublayer): Attention(\n",
       "            (in_proj): AttnInProjV2(\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "            )\n",
       "            (attn): ScaledDotProdAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (scale): Scale()\n",
       "        )\n",
       "        (ff): ReZero(\n",
       "          (sublayer): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.1, inplace=False)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (scale): Scale()\n",
       "        )\n",
       "      )\n",
       "      (1): XEncoderBlock(\n",
       "        (attn): ReZero(\n",
       "          (sublayer): Attention(\n",
       "            (in_proj): AttnInProjV2(\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "            )\n",
       "            (attn): ScaledDotProdAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (scale): Scale()\n",
       "        )\n",
       "        (ff): ReZero(\n",
       "          (sublayer): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.1, inplace=False)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (scale): Scale()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proj): Linear(in_features=512, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XTransformerLM.from_config(XConfig(n_layers=2, residual_type='rezero'))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_attention.ipynb.\n",
      "Converted 03_models.transformer.ipynb.\n",
      "Converted 04a_models.reformer.ipynb.\n",
      "Converted 04x_models.xtransformer.ipynb.\n",
      "Converted 05_tokenizers.ipynb.\n",
      "Converted 06_data.ipynb.\n",
      "Converted 07_metrics.ipynb.\n",
      "Converted 08_optimizers.ipynb.\n",
      "Converted 09_tracking.ipynb.\n",
      "Converted 10_config.ipynb.\n",
      "Converted 40_experimental.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchenv]",
   "language": "python",
   "name": "conda-env-torchenv-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
