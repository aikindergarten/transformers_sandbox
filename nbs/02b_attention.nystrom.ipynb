{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp attention.nystrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from fastai.basics import *\n",
    "\n",
    "from functools import partial, reduce\n",
    "from inspect import isfunction\n",
    "from operator import mul\n",
    "from copy import deepcopy\n",
    "import math\n",
    "from torch import Tensor\n",
    "from typing import Tuple\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from transformers_sandbox.core import *\n",
    "from transformers_sandbox.layers import *\n",
    "from transformers_sandbox.attention.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nystrom Attention\n",
    "> Memory efficient attention computation based on Nystrom aproximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper: https://arxiv.org/abs/2102.03902\n",
    "\n",
    "Authors code: https://github.com/mlpen/Nystromformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class reshape2bhnd:\n",
    "    \"b n (h d) -> b h n d\"\n",
    "    def __init__(self, h=8): self.h = h\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        b, n, d = x.size()\n",
    "        return x.view(b, n, self.h, d//self.h).transpose(1,2)\n",
    "    \n",
    "def reshape2bnd(x):\n",
    "    b, h, n, d = x.size()\n",
    "    return x.transpose(1,2).contiguous().view(b, n, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moore-Penrose iterative pseudoinverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_pinv(A, n_iter=6):\n",
    "    \"Iteratively computes Moore-Penrose pseudoinverse of matrix `A`\"\n",
    "    I = torch.eye(A.size(-1), device=A.device)\n",
    "    #Note: A.abs().sum(-1).max() == 1 because of sofmax applied to A\n",
    "    Z = A.transpose(-1,-2) / A.abs().sum(-2).max(-1).values[..., None, None]\n",
    "    for _ in range(n_iter):\n",
    "        AZ = A@Z\n",
    "        Z = 0.25*Z @ (13*I - AZ @ (15*I - AZ @ (7*I - AZ)))\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(7.8314), 0.0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "x = torch.softmax(torch.randn(64,64), dim=-1)\n",
    "I = torch.eye(x.size(-1))\n",
    "z = x.transpose(-1,-2) / x.abs().sum(-2).max(-1).values[..., None, None]\n",
    "# assert torch.norm((I - x@z)) < 1\n",
    "torch.norm(I - x@z), np.linalg.det(x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.softmax(torch.randn(4,2,64,64), dim=-1)\n",
    "n_iter = 40\n",
    "z = iter_pinv(x, n_iter)\n",
    "assert torch.allclose(x@z, torch.eye(x.size(-1)), atol=1e-4, rtol=1e-4), \\\n",
    "        f\"Iterative pseudoinverse didn't converge in {n_iter} iterations\"\n",
    "# (x@z - torch.eye(x.size(-1))).max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#skip\n",
    "#hide\n",
    "x.requires_grad = True\n",
    "x.pinverse()\n",
    "loss = x.pinverse().sum()\n",
    "loss.backward()\n",
    "x.grad.isnan().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip\n",
    "# %timeit x.pinverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip\n",
    "# %timeit iter_pinv(x, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nystrom attention aproximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* [ ] add masking\n",
    "* [ ] more testcases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mScaledDotProdAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcausal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mshared_qk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstore_attention\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      Computes scaled dot-product attnetion given q, k, v\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           /media/arto/work/dev/git/transformers_sandbox/transformers_sandbox/attention/core.py\n",
       "\u001b[0;31mType:\u001b[0m           PrePostInitMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?ScaledDotProdAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NystromAttention(Module):\n",
    "    \"\"\"Computes attention using nystrom aproximation based approach\"\"\"\n",
    "    def __init__(self, d_model, n_heads=8, causal=False, n_landmarks=64,\n",
    "                 store_attention:bool=False, use_conv=False, dropout=0.,\n",
    "                 pinv_n_iter=6, conv_kernel_size=33, **kwargs):\n",
    "        store_attr()\n",
    "        self.scale = (d_model//n_heads)**-0.5\n",
    "        if use_conv:\n",
    "            self.conv = nn.Conv2d(n_heads, n_heads,\n",
    "                                  kernel_size=(conv_kernel_size, 1),\n",
    "                                  padding=(conv_kernel_size//2, 0),\n",
    "                                  groups=n_heads, bias=False)\n",
    "    \n",
    "    def forward(self, q, k, v, attn_mask=None):\n",
    "        bs, n, d, h, l = *q.size(), self.n_heads, self.n_landmarks\n",
    "        dh = d//h\n",
    "        #reshape = partial(rearrange, pattern='b n (h d) -> b h n d', h=self.n_heads)\n",
    "        q, k, v = map(reshape2bhnd(h), (q,k,v))\n",
    "        \n",
    "        if n <= self.n_landmarks: \n",
    "            #?? mb do this with threshold instead\n",
    "            dots = F.softmax(einsum('...nd, ...md -> ...nm', q*scale, k), dim=-1)\n",
    "            if exists(attn_mask):\n",
    "                dots.masked_fill_(~attn_mask, MASK_VAL)\n",
    "                del attn_mask\n",
    "            out = einsum('...nm, ...md -> ...nd', dots, v)\n",
    "        \n",
    "        ql = torch.reshape(q, (bs, h, l, -1, dh)).mean(dim=-2)\n",
    "        kl = torch.reshape(k, (bs, h, l, -1, dh)).mean(dim=-2)\n",
    "        \n",
    "        f = F.softmax(einsum('...nd, ...md -> ...nm', q *self.scale, kl), dim=-1)\n",
    "        b = F.softmax(einsum('...md, ...nd -> ...mn', ql*self.scale, k ), dim=-1)\n",
    "        a = F.softmax(einsum('...ld, ...md -> ...lm', ql*self.scale, kl), dim=-1)\n",
    "        a = iter_pinv(a)\n",
    "        \n",
    "        out = einsum('...nm, ...md -> ...nd',\n",
    "                     einsum('...nl, ...lm -> ...nm', f, a),\n",
    "                     einsum('...mn, ...nd -> ...md', b, v))\n",
    "        \n",
    "        if self.use_conv:\n",
    "            out += self.conv(v)\n",
    "        return reshape2bnd(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128, 8]) torch.Size([4, 8, 128]) torch.Size([4, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "q = torch.randn(bs, sl, d)\n",
    "k = torch.randn(bs, sl, d)\n",
    "v = torch.randn(bs, sl, d)\n",
    "l = 8 # number of landmark points\n",
    "scale = d**-0.5\n",
    "ql = torch.reshape(q, (bs, l, -1, d)).mean(dim=-2)\n",
    "kl = torch.reshape(k, (bs, l, -1, d)).mean(dim=-2)\n",
    "ql.shape, kl.shape\n",
    "\n",
    "f = F.softmax(einsum('...nd, ...md -> ...nm', q*scale, kl), dim=-1)\n",
    "b = F.softmax(einsum('...md, ...nd -> ...mn', ql*scale, k), dim=-1)\n",
    "print(f.shape, b.shape, end=' ')\n",
    "a = einsum('...nd, ...md -> ...nm', ql*scale, kl)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "q = torch.randn(bs, sl, d)\n",
    "k = torch.randn(bs, sl, d)\n",
    "v = torch.randn(bs, sl, d)\n",
    "attn_func = NystromAttention(d, 4, n_landmarks=16)\n",
    "out = attn_func(q, k, v)\n",
    "assert out.size() == (bs,sl,d)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "attn_func = NystromAttention(d, 4)\n",
    "mask = torch.ones(1,sl,sl).bool()\n",
    "out = attn_func(q, k, v, attn_mask=mask)\n",
    "assert out.size() == (bs,sl,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuda\n",
    "q = torch.randn(bs, sl, d).cuda()\n",
    "k = torch.randn(bs, sl, d).cuda()\n",
    "v = torch.randn(bs, sl, d).cuda()\n",
    "attn_func = NystromAttention(d, 4)\n",
    "out = attn_func(q, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#skip\n",
    "NystromerAttention = partial(Attention, attn_func=NystromAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip\n",
    "#hide\n",
    "class _Attention(Module):\n",
    "    \"\"\"\n",
    "    Standard attention module using scaled dot-product attention\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model:int, \n",
    "                 n_heads:int = 8, \n",
    "                 causal:bool = False,\n",
    "                 mask:Tensor = None,\n",
    "                 dropout:float=0.1,\n",
    "                 out_dropout:float=None,\n",
    "                 bias:bool=False,\n",
    "                 shared_qk:bool=False,\n",
    "                 store_attention:bool=False):\n",
    "        store_attr('causal, mask, n_heads, bias, shared_qk')\n",
    "        out_dropout = ifnone(out_dropout, dropout)\n",
    "        if shared_qk: self.in_proj = SharedQKAttnInProj(d_model, bias=bias)\n",
    "        else: self.in_proj = AttnInProjV2(d_model, bias=bias)\n",
    "        self.attn = ScaledDotProdAttention(d_model, n_heads, causal=causal,\n",
    "                                           dropout=dropout, shared_qk=shared_qk, \n",
    "                                           store_attention=store_attention)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.dropout = nn.Dropout(out_dropout)\n",
    "        self._init()\n",
    "\n",
    "    def forward(self, x, context = None, mask = None, context_mask = None):\n",
    "        q, k, v = self.in_proj(x, context)\n",
    "        if self.shared_qk: k = F.normalize(k, 2, dim=-1).type_as(k)\n",
    "                \n",
    "        attn_mask = self._make_attn_mask(mask, context_mask, x, context)\n",
    "        out = self.attn(q, k, v, attn_mask)\n",
    "        \n",
    "        out = self.out_proj(out)\n",
    "        return self.dropout(out)\n",
    "        \n",
    "    def _init(self):\n",
    "        [nn.init.xavier_uniform_(w) for w in self.parameters() if w.dim()>1]\n",
    "        if self.bias:\n",
    "            [nn.init.constant_(b, 0) for b in self.parameters() if b.dim()==1]\n",
    "    \n",
    "    def _make_attn_mask(self, mask, context_mask, x, context):\n",
    "        if any(map(exists, (mask, context_mask))):\n",
    "            b, n, _, device = *x.size(), x.device\n",
    "            q_mask = default(mask, lambda: torch.ones((b, n), device = device).bool())\n",
    "            k_mask = q_mask if not exists(context) else context_mask\n",
    "            k_mask = default(k_mask, lambda: torch.ones((b, context.shape[-2]), device = device).bool())\n",
    "            \n",
    "            q_mask = rearrange(q_mask, 'b i -> b () i ()')\n",
    "            k_mask = rearrange(k_mask, 'b j -> b () () j')\n",
    "            return q_mask * k_mask\n",
    "        else: return None #attn_mask is None if both mask and context_mask are None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "context = torch.randn(bs, sl-16, d)\n",
    "attn = NystromerAttention(d)\n",
    "out = attn(x)\n",
    "assert (bs, sl, d) == out.size()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = attn(x, context)\n",
    "# assert (bs, sl, d) == out.size()\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e_msg = \"Causal masking error\"\n",
    "# attn = Attention(d, causal=True, dropout=0)\n",
    "# x1 = torch.randn(bs, sl, d)\n",
    "# out1 = attn(x1)\n",
    "# x2 = x1.clone()\n",
    "# x2[:, sl//2:, :] = torch.randn(bs, sl//2, d)\n",
    "# out2 = attn(x2)\n",
    "# # all elements in first half are equal despite second half is defferent\n",
    "# assert all_equal(out1[:, :sl//2], out2[:, :sl//2]), e_msg\n",
    "# assert not (out1[:, sl//2:] == out2[:, sl//2:]).any(), e_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e_msg = \"Masking error\"\n",
    "# attn = Attention(d, causal=False, dropout=0)\n",
    "# x1 = torch.randn(bs, sl, d)\n",
    "# mask = torch.ones(bs, sl)\n",
    "# # mask out second half of input\n",
    "# mask[:, sl//2:] = 0\n",
    "# mask = mask.bool()\n",
    "# out1 = attn(x1, mask=mask)\n",
    "# x2 = x1.clone()\n",
    "# x2[:, sl//2:, :] = torch.randn(bs, sl//2, d)\n",
    "# out2 = attn(x2, mask=mask)\n",
    "# # all elements are equal, masked values do not effect result\n",
    "# assert all_equal(out1[:, :sl//2], out2[:, :sl//2]), e_msg\n",
    "# out1 = attn(x1)\n",
    "# out2 = attn(x2)\n",
    "# assert not (out1[:, :sl//2] == out2[:, :sl//2]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e_msg = \"Context masking error\"\n",
    "# attn = Attention(d, causal=False, dropout=0)\n",
    "# x = torch.randn(bs, sl, d)\n",
    "# context = torch.randn(bs, sl, d)\n",
    "# context_mask = torch.ones(bs, sl)\n",
    "# # mask out second half of context\n",
    "# context_mask[:, sl//2:] = 0\n",
    "# context_mask = context_mask.bool()\n",
    "# out1 = attn(x, context, context_mask=context_mask)\n",
    "# context2 = context.clone()\n",
    "# context2[:, sl//2:, :] = torch.randn(bs, sl//2, d)\n",
    "# out2 = attn(x, context2, context_mask=context_mask)\n",
    "# # all elements are equal, masked values do not effect result\n",
    "# assert all_equal(out1, out2), e_msg\n",
    "# # all output values are different for different context\n",
    "# out1 = attn(x, context)\n",
    "# out2 = attn(x, context2)\n",
    "# assert not (out1 == out2).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check stored attention matrix\n",
    "# torch.manual_seed(842)\n",
    "# bs = 4\n",
    "# sl = 16\n",
    "# csl = sl + 16\n",
    "# d = 64\n",
    "# x = torch.rand(bs, sl, d)\n",
    "# context = torch.rand(bs, csl, d)\n",
    "# mask = torch.ones(bs, sl)\n",
    "# mask[:, -5:] = 0\n",
    "# context_mask = torch.ones(bs, csl)\n",
    "# context_mask[:, -10:] = 0\n",
    "# mask, context_mask = mask.bool(), context_mask.bool()\n",
    "# attn = Attention(d, store_attention=True)\n",
    "# out = attn(x, context, mask=mask, context_mask=context_mask)\n",
    "# attention = attn.attn.attention\n",
    "# assert (bs, sl, d) == out.size()\n",
    "# assert attention.size() == (bs, attn.attn.n_heads, sl, csl)\n",
    "# # zeros for masked keys and \"don't cares\" for masked queries\n",
    "# plt.matshow(attention[0,0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #hide\n",
    "# #skip\n",
    "# # check stored attention matrix\n",
    "# torch.manual_seed(842)\n",
    "# bs = 4\n",
    "# sl = 16\n",
    "# d = 64\n",
    "# x = torch.rand(bs, sl, d)\n",
    "# mask = torch.ones(bs, sl)\n",
    "# mask[:, -5:] = 0\n",
    "# mask = mask.bool()\n",
    "# attn = Attention(d, store_attention=True, causal=True)\n",
    "# out = attn(x, mask=mask)\n",
    "# attention = attn.attn.attention\n",
    "# assert (bs, sl, d) == out.size()\n",
    "# assert attention.size() == (bs, attn.attn.n_heads, sl, sl)\n",
    "# # zeros for masked keys and \"don't cares\" for masked queries\n",
    "# plt.matshow(attention[0,0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_attention.core.ipynb.\n",
      "Converted 02b_attention.nystrom.ipynb.\n",
      "Converted 03_models.transformer.ipynb.\n",
      "Converted 04a_models.reformer.ipynb.\n",
      "Converted 04x_models.xtransformer.ipynb.\n",
      "Converted 05_tokenizers.ipynb.\n",
      "Converted 06_data.ipynb.\n",
      "Converted 07_metrics.ipynb.\n",
      "Converted 08_optimizers.ipynb.\n",
      "Converted 09_tracking.ipynb.\n",
      "Converted 10_config.ipynb.\n",
      "Converted 40_experimental.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchenv]",
   "language": "python",
   "name": "conda-env-torchenv-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
