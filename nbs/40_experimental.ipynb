{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.text.all import *\n",
    "from fastai.callback import *\n",
    "\n",
    "from transformers_sandbox.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental blocks and features\n",
    "> Place where things develope before departing to relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CharLMConfig(ConfigBase):\n",
    "    \"Config for quick char-level LM experiments\"\n",
    "    _model = TransformerLM\n",
    "    _d = {\n",
    "        'vocab_sz':256,\n",
    "        'd_model':512,\n",
    "        'n_layers':6,\n",
    "        'n_heads':8,\n",
    "        'd_ff':4096,\n",
    "        'attn_dropout':0.1,\n",
    "        'ff_dropout':0.1,\n",
    "        'emb_dropout':0.1,\n",
    "        'tie_weights':True,\n",
    "        'causal':True,\n",
    "        'pos_enc':'absolute',\n",
    "        'max_seq_len':512,\n",
    "        'axial_shape':None,\n",
    "        'axial_emb_dims':None,\n",
    "        'pad_idx':None,\n",
    "        'prenorm':False,\n",
    "        'attn_bias':False,\n",
    "        'shared_qk':False,\n",
    "    }\n",
    "    @update_sig(_d)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FixUp init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer w/o LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerEncoderBlockNLN(Module):\n",
    "    \"\"\"\n",
    "    tmp\n",
    "    Bacis transformer encoder block. Consists of multi-head attention and positional \n",
    "    feedforward layers\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_model:int, \n",
    "                 n_heads:int = 8, \n",
    "                 d_ff:int = None, \n",
    "                 attn_dropout:float = 0.1,\n",
    "                 ff_dropout:float = 0.1,\n",
    "                 causal:bool = False, \n",
    "                 attn_bias:bool = False,\n",
    "                 prenorm:bool=False,\n",
    "                 shared_qk:bool=False):\n",
    "        store_attr('attn_dropout') # mb separate argument attn_post_dropout\n",
    "        self.attn = Residual(Attention(d_model, n_heads=n_heads, causal=causal, dropout=attn_dropout, bias=attn_bias, shared_qk=shared_qk))\n",
    "        self.ff = Residual(FeedForward(d_model, d_ff=d_ff, dropout=ff_dropout))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        out = self.attn(x, mask=mask)\n",
    "        return self.ff(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "m = TransformerEncoderBlockNLN(d)\n",
    "out = m(x)\n",
    "assert (out.size() == (bs, sl, d))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerEncoderNLN(Module):\n",
    "    \"\"\"Stack of TransformerEncoderBlocks\"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 n_layers=6, \n",
    "                 n_heads=8, \n",
    "                 d_ff=None,\n",
    "                 ff_dropout=0.1, \n",
    "                 attn_dropout=0.1,\n",
    "                 attn_bias=False,\n",
    "                 causal=False, \n",
    "                 prenorm=False,\n",
    "                 shared_qk:bool=False,\n",
    "                 final_norm=None):\n",
    "        store_attr('d_model')\n",
    "        self.layers = nn.ModuleList([])    \n",
    "        for _ in range(n_layers):\n",
    "            self.layers.append(TransformerEncoderBlockNLN(d_model, n_heads, causal=causal, \n",
    "                                    d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout, \n",
    "                                    prenorm=prenorm, attn_bias=attn_bias, shared_qk=shared_qk))\n",
    "        self.norm = None if final_norm is None else final_norm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers: x = layer(x, mask=mask)\n",
    "        if self.norm is not None: x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(bs, sl, d)\n",
    "m = TransformerEncoderNLN(d, n_layers=2)\n",
    "out = m(x)\n",
    "assert (out.size() == (bs, sl, d))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoderNLN(\n",
       "  (layers): ModuleList(\n",
       "    (0): TransformerEncoderBlockNLN(\n",
       "      (attn): Residual(\n",
       "        (sublayer): Attention(\n",
       "          (in_proj): AttnInProjV2(\n",
       "            (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (to_kv): Linear(in_features=64, out_features=128, bias=False)\n",
       "          )\n",
       "          (attn): ScaledDotProdAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (out_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): Residual(\n",
       "        (sublayer): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerEncoderBlockNLN(\n",
       "      (attn): Residual(\n",
       "        (sublayer): Attention(\n",
       "          (in_proj): AttnInProjV2(\n",
       "            (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (to_kv): Linear(in_features=64, out_features=128, bias=False)\n",
       "          )\n",
       "          (attn): ScaledDotProdAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (out_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): Residual(\n",
       "        (sublayer): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerLMNLN(Module, LMMixin):\n",
    "    \"\"\"\n",
    "    tmp\n",
    "    Basic Transformer for language modelling\n",
    "    \n",
    "    Parameters:\n",
    "        * vocab_sz: int\n",
    "        * d_model: int - inner dimension of the model\n",
    "        * n_layers: int (default: 6) \n",
    "        * n_heads: int (default: 8)\n",
    "        * d_ff: int - inner dimension of the pointwise FeedForward net, if None defaults to 4*d_model\n",
    "        * attn_dropout: float - attention dropout\n",
    "        * ff_dropout: float - feed-forward dropout\n",
    "        * emb_dropout: float - embedding dropout\n",
    "        * causal: bool (default: True) - if True does causal masking automatically\n",
    "        * max_seq_len: int (default: 512)\n",
    "        * tie_weights: bool - if True target embedding weights are used for computation output projection\n",
    "        * prenorm: bool - wether to use PreNorm or PostNorm\n",
    "        * attn_bias: bool - wether to allow biases in attention projection layers\n",
    "        * pad_idx: int - padding token id, required for autogeneration of padding mask\n",
    "        * pos_enc: str from {'absolute', 'fixed', 'axial'} - type of positional encoding to use\n",
    "        * axial_shape: tuple - [optional] should be factors of max_seq_len\n",
    "        * axial_emb_dims: tuple - [optional] axial embedding components, should sum to d_model\n",
    "    Inputs:\n",
    "        * x - input ids, shape [bs, sl]\n",
    "        * mask - optional boolean mask, shape [bs, sl]\n",
    "    Returns:\n",
    "        * logits - target token logits, shape [bs, sl, vocab_sz]\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 vocab_sz:int, \n",
    "                 d_model:int, \n",
    "                 n_layers:int=6,\n",
    "                 n_heads:int=8,\n",
    "                 d_ff:int=None,\n",
    "                 attn_dropout:float=0.1,\n",
    "                 ff_dropout:float=0.1,\n",
    "                 emb_dropout:float=0.1,\n",
    "                 tie_weights:bool=True,\n",
    "                 causal:bool=True,\n",
    "                 pos_enc:str='absolute',\n",
    "                 max_seq_len:int=512,\n",
    "                 axial_shape:tuple=None,\n",
    "                 axial_emb_dims:tuple=None,\n",
    "                 pad_idx:int=None,\n",
    "                 prenorm:bool=False,\n",
    "                 attn_bias:bool=False,\n",
    "                 shared_qk:bool=False):\n",
    "        store_attr()\n",
    "        self.emb = TransformerEmbedding(vocab_sz, d_model, max_seq_len, dropout=emb_dropout, \n",
    "                                        pos_enc=pos_enc, axial_shape=axial_shape, \n",
    "                                        axial_emb_dims=axial_emb_dims)\n",
    "        final_norm = None\n",
    "        self.encoder = TransformerEncoderNLN(d_model, n_layers, n_heads, causal=causal, d_ff=d_ff,\n",
    "                                          attn_dropout=attn_dropout, ff_dropout=ff_dropout,\n",
    "                                          prenorm=prenorm, attn_bias=attn_bias,\n",
    "                                          shared_qk=shared_qk, final_norm=final_norm)\n",
    "        self.proj = nn.Linear(d_model, vocab_sz)\n",
    "        if tie_weights: self.proj.weight = self.emb.emb.weight\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.emb(x)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "vocab_sz = 256\n",
    "x = torch.randint(vocab_sz, (bs, sl))\n",
    "model = TransformerLMNLN(vocab_sz, d, n_layers=2, causal=True)\n",
    "out = model(x)\n",
    "assert (out.size() == (bs, sl, vocab_sz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fixup_init(model):\n",
    "    \"Applies FixUp initialization to LM (proto ver)\"\n",
    "    n_blocks = len(model.encoder.layers)*2\n",
    "    for l in model.encoder.layers:\n",
    "        l.attn.sublayer.in_proj.to_q.weight.data *= n_blocks**(-1/2)\n",
    "        l.attn.sublayer.in_proj.to_kv.weight.data *= n_blocks**(-1/4)\n",
    "        l.attn.sublayer.out_proj.weight.data *= 0.\n",
    "\n",
    "        l.ff.sublayer.net[0].weight.data *= n_blocks**-0.5\n",
    "        l.ff.sublayer.net[0].bias.data.zero_()\n",
    "        l.ff.sublayer.net[3].weight.data.zero_()\n",
    "        l.ff.sublayer.net[3].bias.data.zero_()\n",
    "\n",
    "    model.proj.weight.data.zero_()\n",
    "    model.proj.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scales and Shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Scale(Module):\n",
    "    def  __init__(self, scale=1.):\n",
    "        self.scale = torch.nn.Parameter(torch.ones(1)*scale)\n",
    "    def forward(self, x):\n",
    "        return x * self.scale\n",
    "\n",
    "class Shift(Module):\n",
    "    def __init__(self):\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(1))\n",
    "    def forward(self, x):\n",
    "        return x + self.bias\n",
    "\n",
    "class ShiftScale(Module):\n",
    "    def __init__(self, sublayer, scale=1.):\n",
    "        self.sublayer = sublayer\n",
    "        self.shift = Shift()\n",
    "        self.scale = Scale()\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.shift(x)\n",
    "        x = self.sublayer(x, **kwargs)\n",
    "        return self.scale(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardFixup(Module):\n",
    "    \"\"\"\n",
    "    FeedForward with shifts and scale for FixUp\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model:int, d_ff:int=None, dropout:float=0.):\n",
    "        d_ff = default(d_ff, 4 * d_model)\n",
    "        layers = OrderedDict(\n",
    "            [('shift1',Shift()),\n",
    "            ('fc1',nn.Linear(d_model, d_ff)),\n",
    "            ('shift2',Shift()),\n",
    "            ('act',nn.GELU()),\n",
    "            ('dropout1',nn.Dropout(dropout)),\n",
    "            ('shift3',Shift()),\n",
    "            ('fc2',nn.Linear(d_ff, d_model)),\n",
    "            ('dropout2',nn.Dropout(dropout)),\n",
    "            ('scale',Scale())])\n",
    "        self.net = nn.Sequential(layers)\n",
    "        self._init()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def _init(self):\n",
    "        [nn.init.xavier_uniform_(p) for p in self.parameters() if p.dim() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerEncoderBlockNLN2(Module):\n",
    "    \"\"\"\n",
    "    tmp\n",
    "    Bacis transformer encoder block. Consists of multi-head attention and positional \n",
    "    feedforward layers\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_model:int, \n",
    "                 n_heads:int = 8, \n",
    "                 d_ff:int = None, \n",
    "                 attn_dropout:float = 0.1,\n",
    "                 ff_dropout:float = 0.1,\n",
    "                 causal:bool = False, \n",
    "                 attn_bias:bool = False,\n",
    "                 prenorm:bool=False,\n",
    "                 shared_qk:bool=False):\n",
    "        store_attr('attn_dropout') # mb separate argument attn_post_dropout\n",
    "        self.attn = Residual(ShiftScale(Attention(d_model, n_heads=n_heads, causal=causal, dropout=attn_dropout, bias=attn_bias, shared_qk=shared_qk)))\n",
    "        self.ff = Residual(FeedForwardFixup(d_model, d_ff=d_ff, dropout=ff_dropout))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        out = self.attn(x, mask=mask)\n",
    "        return self.ff(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "m = TransformerEncoderBlockNLN2(d)\n",
    "out = m(x)\n",
    "assert (out.size() == (bs, sl, d))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerEncoderNLN2(Module):\n",
    "    \"\"\"Stack of TransformerEncoderBlocks\"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 n_layers=6, \n",
    "                 n_heads=8, \n",
    "                 d_ff=None,\n",
    "                 ff_dropout=0.1, \n",
    "                 attn_dropout=0.1,\n",
    "                 attn_bias=False,\n",
    "                 causal=False, \n",
    "                 prenorm=False,\n",
    "                 shared_qk:bool=False,\n",
    "                 final_norm=None):\n",
    "        store_attr('d_model')\n",
    "        self.layers = nn.ModuleList([])    \n",
    "        for _ in range(n_layers):\n",
    "            self.layers.append(TransformerEncoderBlockNLN2(d_model, n_heads, causal=causal, \n",
    "                                    d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout, \n",
    "                                    prenorm=prenorm, attn_bias=attn_bias, shared_qk=shared_qk))\n",
    "        self.norm = None if final_norm is None else final_norm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers: x = layer(x, mask=mask)\n",
    "        if self.norm is not None: x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(bs, sl, d)\n",
    "m = TransformerEncoderNLN2(d, n_layers=2)\n",
    "out = m(x)\n",
    "assert (out.size() == (bs, sl, d))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoderNLN2(\n",
       "  (layers): ModuleList(\n",
       "    (0): TransformerEncoderBlockNLN2(\n",
       "      (attn): Residual(\n",
       "        (sublayer): ShiftScale(\n",
       "          (sublayer): Attention(\n",
       "            (in_proj): AttnInProjV2(\n",
       "              (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
       "              (to_kv): Linear(in_features=64, out_features=128, bias=False)\n",
       "            )\n",
       "            (attn): ScaledDotProdAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (out_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (shift): Shift()\n",
       "          (scale): Scale()\n",
       "        )\n",
       "      )\n",
       "      (ff): Residual(\n",
       "        (sublayer): FeedForwardFixup(\n",
       "          (net): Sequential(\n",
       "            (shift1): Shift()\n",
       "            (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (shift2): Shift()\n",
       "            (act): GELU()\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (shift3): Shift()\n",
       "            (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (scale): Scale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerEncoderBlockNLN2(\n",
       "      (attn): Residual(\n",
       "        (sublayer): ShiftScale(\n",
       "          (sublayer): Attention(\n",
       "            (in_proj): AttnInProjV2(\n",
       "              (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
       "              (to_kv): Linear(in_features=64, out_features=128, bias=False)\n",
       "            )\n",
       "            (attn): ScaledDotProdAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (out_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (shift): Shift()\n",
       "          (scale): Scale()\n",
       "        )\n",
       "      )\n",
       "      (ff): Residual(\n",
       "        (sublayer): FeedForwardFixup(\n",
       "          (net): Sequential(\n",
       "            (shift1): Shift()\n",
       "            (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (shift2): Shift()\n",
       "            (act): GELU()\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (shift3): Shift()\n",
       "            (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (scale): Scale()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerLMNLN2(Module, LMMixin):\n",
    "    \"\"\"\n",
    "    tmp\n",
    "    Basic Transformer for language modelling\n",
    "    \n",
    "    Parameters:\n",
    "        * vocab_sz: int\n",
    "        * d_model: int - inner dimension of the model\n",
    "        * n_layers: int (default: 6) \n",
    "        * n_heads: int (default: 8)\n",
    "        * d_ff: int - inner dimension of the pointwise FeedForward net, if None defaults to 4*d_model\n",
    "        * attn_dropout: float - attention dropout\n",
    "        * ff_dropout: float - feed-forward dropout\n",
    "        * emb_dropout: float - embedding dropout\n",
    "        * causal: bool (default: True) - if True does causal masking automatically\n",
    "        * max_seq_len: int (default: 512)\n",
    "        * tie_weights: bool - if True target embedding weights are used for computation output projection\n",
    "        * prenorm: bool - wether to use PreNorm or PostNorm\n",
    "        * attn_bias: bool - wether to allow biases in attention projection layers\n",
    "        * pad_idx: int - padding token id, required for autogeneration of padding mask\n",
    "        * pos_enc: str from {'absolute', 'fixed', 'axial'} - type of positional encoding to use\n",
    "        * axial_shape: tuple - [optional] should be factors of max_seq_len\n",
    "        * axial_emb_dims: tuple - [optional] axial embedding components, should sum to d_model\n",
    "    Inputs:\n",
    "        * x - input ids, shape [bs, sl]\n",
    "        * mask - optional boolean mask, shape [bs, sl]\n",
    "    Returns:\n",
    "        * logits - target token logits, shape [bs, sl, vocab_sz]\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 vocab_sz:int, \n",
    "                 d_model:int, \n",
    "                 n_layers:int=6,\n",
    "                 n_heads:int=8,\n",
    "                 d_ff:int=None,\n",
    "                 attn_dropout:float=0.1,\n",
    "                 ff_dropout:float=0.1,\n",
    "                 emb_dropout:float=0.1,\n",
    "                 tie_weights:bool=True,\n",
    "                 causal:bool=True,\n",
    "                 pos_enc:str='absolute',\n",
    "                 max_seq_len:int=512,\n",
    "                 axial_shape:tuple=None,\n",
    "                 axial_emb_dims:tuple=None,\n",
    "                 pad_idx:int=None,\n",
    "                 prenorm:bool=False,\n",
    "                 attn_bias:bool=False,\n",
    "                 shared_qk:bool=False):\n",
    "        store_attr()\n",
    "        self.emb = TransformerEmbedding(vocab_sz, d_model, max_seq_len, dropout=emb_dropout, \n",
    "                                        pos_enc=pos_enc, axial_shape=axial_shape, \n",
    "                                        axial_emb_dims=axial_emb_dims)\n",
    "        final_norm = None\n",
    "        self.encoder = TransformerEncoderNLN2(d_model, n_layers, n_heads, causal=causal, d_ff=d_ff,\n",
    "                                          attn_dropout=attn_dropout, ff_dropout=ff_dropout,\n",
    "                                          prenorm=prenorm, attn_bias=attn_bias,\n",
    "                                          shared_qk=shared_qk, final_norm=final_norm)\n",
    "        self.proj = nn.Linear(d_model, vocab_sz)\n",
    "        if tie_weights: self.proj.weight = self.emb.emb.weight\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.emb(x)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "vocab_sz = 256\n",
    "x = torch.randint(vocab_sz, (bs, sl))\n",
    "model = TransformerLMNLN2(vocab_sz, d, n_layers=2, causal=True)\n",
    "out = model(x)\n",
    "assert (out.size() == (bs, sl, vocab_sz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fixup_init2(model):\n",
    "    \"Applies FixUp initialization to LM (proto ver)\"\n",
    "    n_blocks = len(model.encoder.layers)*2\n",
    "    for l in model.encoder.layers:\n",
    "        #?? is -1/6 right or should be -1/2;-1/4\n",
    "        l.attn.sublayer.sublayer.in_proj.to_q.weight.data *= n_blocks**(-1/6)\n",
    "        l.attn.sublayer.sublayer.in_proj.to_kv.weight.data *= n_blocks**(-1/6)\n",
    "        l.attn.sublayer.sublayer.out_proj.weight.data *= 0.\n",
    "\n",
    "        l.ff.sublayer.net.fc1.weight.data *= n_blocks**-0.5\n",
    "        l.ff.sublayer.net.fc1.bias.data.zero_()\n",
    "        l.ff.sublayer.net.fc2.weight.data.zero_()\n",
    "        l.ff.sublayer.net.fc2.bias.data.zero_()\n",
    "\n",
    "    model.proj.weight.data.zero_()\n",
    "    model.proj.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADMIN init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AdminResidual(Module):\n",
    "    def __init__(self, sublayer, d_model):\n",
    "        self.sublayer = sublayer\n",
    "        self.w = torch.nn.Parameter(torch.ones(d_model))\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return x*self.w + self.sublayer(x, *args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerEncoderBlockAdmin(Module):\n",
    "    \"\"\"\n",
    "    Bacis transformer encoder block. Consists of multi-head attention and positional \n",
    "    feedforward layers\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_model:int, \n",
    "                 n_heads:int = 8, \n",
    "                 d_ff:int = None, \n",
    "                 attn_dropout:float = 0.1,\n",
    "                 ff_dropout:float = 0.1,\n",
    "                 causal:bool = False, \n",
    "                 attn_bias:bool = False, \n",
    "                 prenorm:bool=False,\n",
    "                 shared_qk:bool=False):\n",
    "        store_attr('attn_dropout') # mb separate argument attn_post_dropout\n",
    "        \n",
    "        self.attn = PostNorm(d_model, AdminResidual(Attention(d_model, n_heads=n_heads, causal=causal, dropout=attn_dropout, bias=attn_bias, shared_qk=shared_qk), d_model))\n",
    "        self.ff = PostNorm(d_model, AdminResidual(FeedForward(d_model, d_ff=d_ff, dropout=ff_dropout), d_model))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        out = self.attn(x, mask=mask)\n",
    "        return self.ff(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "m = TransformerEncoderBlockAdmin(d)\n",
    "out = m(x)\n",
    "assert (out.size() == (bs, sl, d))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerEncoderAdmin(Module):\n",
    "    \"\"\"Stack of TransformerEncoderBlocks\"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 n_layers=6, \n",
    "                 n_heads=8, \n",
    "                 d_ff=None,\n",
    "                 ff_dropout=0.1, \n",
    "                 attn_dropout=0.1,\n",
    "                 attn_bias=False,\n",
    "                 causal=False, \n",
    "                 prenorm=False,\n",
    "                 shared_qk:bool=False,\n",
    "                 final_norm=None):\n",
    "        store_attr('d_model')\n",
    "        self.layers = nn.ModuleList([])    \n",
    "        for _ in range(n_layers):\n",
    "            self.layers.append(TransformerEncoderBlockAdmin(d_model, n_heads, causal=causal, \n",
    "                                    d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout, \n",
    "                                    prenorm=prenorm, attn_bias=attn_bias, shared_qk=shared_qk))\n",
    "        self.norm = None if final_norm is None else final_norm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers: x = layer(x, mask=mask)\n",
    "        if self.norm is not None: x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerLMAdmin(Module, LMMixin):\n",
    "    \"\"\"\n",
    "    tmp\n",
    "    Basic Transformer for language modelling\n",
    "    \n",
    "    Parameters:\n",
    "        * vocab_sz: int\n",
    "        * d_model: int - inner dimension of the model\n",
    "        * n_layers: int (default: 6) \n",
    "        * n_heads: int (default: 8)\n",
    "        * d_ff: int - inner dimension of the pointwise FeedForward net, if None defaults to 4*d_model\n",
    "        * attn_dropout: float - attention dropout\n",
    "        * ff_dropout: float - feed-forward dropout\n",
    "        * emb_dropout: float - embedding dropout\n",
    "        * causal: bool (default: True) - if True does causal masking automatically\n",
    "        * max_seq_len: int (default: 512)\n",
    "        * tie_weights: bool - if True target embedding weights are used for computation output projection\n",
    "        * prenorm: bool - wether to use PreNorm or PostNorm\n",
    "        * attn_bias: bool - wether to allow biases in attention projection layers\n",
    "        * pad_idx: int - padding token id, required for autogeneration of padding mask\n",
    "        * pos_enc: str from {'absolute', 'fixed', 'axial'} - type of positional encoding to use\n",
    "        * axial_shape: tuple - [optional] should be factors of max_seq_len\n",
    "        * axial_emb_dims: tuple - [optional] axial embedding components, should sum to d_model\n",
    "    Inputs:\n",
    "        * x - input ids, shape [bs, sl]\n",
    "        * mask - optional boolean mask, shape [bs, sl]\n",
    "    Returns:\n",
    "        * logits - target token logits, shape [bs, sl, vocab_sz]\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 vocab_sz:int, \n",
    "                 d_model:int, \n",
    "                 n_layers:int=6,\n",
    "                 n_heads:int=8,\n",
    "                 d_ff:int=None,\n",
    "                 attn_dropout:float=0.1,\n",
    "                 ff_dropout:float=0.1,\n",
    "                 emb_dropout:float=0.1,\n",
    "                 tie_weights:bool=True,\n",
    "                 causal:bool=True,\n",
    "                 pos_enc:str='absolute',\n",
    "                 max_seq_len:int=512,\n",
    "                 axial_shape:tuple=None,\n",
    "                 axial_emb_dims:tuple=None,\n",
    "                 pad_idx:int=None,\n",
    "                 prenorm:bool=False,\n",
    "                 attn_bias:bool=False,\n",
    "                 shared_qk:bool=False):\n",
    "        store_attr()\n",
    "        self.emb = TransformerEmbedding(vocab_sz, d_model, max_seq_len, dropout=emb_dropout, \n",
    "                                        pos_enc=pos_enc, axial_shape=axial_shape, \n",
    "                                        axial_emb_dims=axial_emb_dims)\n",
    "        final_norm = None\n",
    "        self.encoder = TransformerEncoderAdmin(d_model, n_layers, n_heads, causal=causal, d_ff=d_ff,\n",
    "                                          attn_dropout=attn_dropout, ff_dropout=ff_dropout,\n",
    "                                          prenorm=prenorm, attn_bias=attn_bias,\n",
    "                                          shared_qk=shared_qk, final_norm=final_norm)\n",
    "        self.proj = nn.Linear(d_model, vocab_sz)\n",
    "        if tie_weights: self.proj.weight = self.emb.emb.weight\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.emb(x)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return self.proj(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BreakFitCallback(Callback):\n",
    "    order=-1\n",
    "    \"Cancels fit after one batch before weight update\"\n",
    "    def before_step(self):\n",
    "        self.model.zero_grad(set_to_none=True)\n",
    "        raise CancelStepException\n",
    "    def after_step(self):\n",
    "        raise CancelBatchException\n",
    "    def after_batch(self):\n",
    "        print('Fit canceled')\n",
    "        raise CancelFitException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def res_submodules(model):\n",
    "    return [m.sublayer for m in learn.model.modules() if isinstance(m, AdminResidual)]\n",
    "\n",
    "def res_modules(model):\n",
    "    return [m for m in learn.model.modules() if isinstance(m, AdminResidual)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip\n",
    "#...\n",
    "# config = CharLMConfig(d_model=512, n_layers=6, max_seq_len=512,\n",
    "#                       pad_idx=pad_id)\n",
    "\n",
    "# learn = Learner(dls, TransformerLMAdmin.from_config(config),\n",
    "#                 loss_func=CrossEntropyLossFlat(ignore_index=pad_id),\n",
    "#                 cbs = [GradientClip(1.0),\n",
    "#                        SaveModelCallback(with_opt=True)],\n",
    "#                 metrics=[accuracy, perplexity, bpc]).to_fp16()\n",
    "# learn.add_cb(ActivationStats(modules=res_submodules(learn.model)))\n",
    "# len(learn.activation_stats.modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with learn.added_cbs(BreakFitCallback()), learn.removed_cbs(SaveModelCallback):\n",
    "#     learn.fit(1, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.activation_stats.stats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def variances(learn):\n",
    "    return np.array([stat['std']**2 for stat in learn.activation_stats.stats[0]])\n",
    "\n",
    "# variances(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _init_scales(vars):\n",
    "    return np.sqrt(np.cumsum(vars))\n",
    "# scales = _init_scales(variances(learn))\n",
    "# scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def admin_init(model, scales):\n",
    "    ms = res_modules(model)\n",
    "    for m, s in zip(ms, scales):\n",
    "        m.w.data *= s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_attention.ipynb.\n",
      "Converted 03_transformer.ipynb.\n",
      "Converted 04_reformer.ipynb.\n",
      "Converted 05_tokenizers.ipynb.\n",
      "Converted 06_data.ipynb.\n",
      "Converted 07_metrics.ipynb.\n",
      "Converted 08_optimizers.ipynb.\n",
      "Converted 09_tracking.ipynb.\n",
      "Converted 10_config.ipynb.\n",
      "Converted 40_experimental.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchenv]",
   "language": "python",
   "name": "conda-env-torchenv-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
