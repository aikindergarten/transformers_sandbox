# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/40_experimental.ipynb (unless otherwise specified).

__all__ = ['CharLMConfig', 'TransformerEncoderBlockNLN', 'TransformerEncoderNLN', 'TransformerLMNLN', 'fixup_init',
           'Scale', 'Shift', 'ShiftScale', 'TransformerEncoderBlockNLN2', 'TransformerEncoderNLN2', 'TransformerLMNLN2',
           'fixup_init2', 'AdminResidual', 'TransformerEncoderBlockAdmin', 'TransformerEncoderAdmin',
           'TransformerLMAdmin', 'BreakFitCallback', 'res_submodules', 'res_modules', 'variances', 'admin_init']

# Cell
from fastai.text.all import *
from fastai.callback import *

from .all import *

# Cell
class CharLMConfig(ConfigBase):
    "Config for quick char-level LM experiments"
    _model = TransformerLM
    _d = {
        'vocab_sz':256,
        'd_model':512,
        'n_layers':6,
        'n_heads':8,
        'd_ff':4096,
        'attn_dropout':0.1,
        'ff_dropout':0.1,
        'emb_dropout':0.1,
        'tie_weights':True,
        'causal':True,
        'pos_enc':'absolute',
        'max_seq_len':512,
        'axial_shape':None,
        'axial_emb_dims':None,
        'pad_idx':None,
        'prenorm':False,
        'attn_bias':False,
        'shared_qk':False,
    }
    @update_sig(_d)
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

# Cell
class TransformerEncoderBlockNLN(Module):
    """
    tmp
    Bacis transformer encoder block. Consists of multi-head attention and positional
    feedforward layers
    """
    def __init__(self,
                 d_model:int,
                 n_heads:int = 8,
                 d_ff:int = None,
                 attn_dropout:float = 0.1,
                 ff_dropout:float = 0.1,
                 causal:bool = False,
                 attn_bias:bool = False,
                 prenorm:bool=False,
                 shared_qk:bool=False):
        store_attr('attn_dropout') # mb separate argument attn_post_dropout
        self.attn = Residual(Attention(d_model, n_heads=n_heads, causal=causal, dropout=attn_dropout, bias=attn_bias, shared_qk=shared_qk))
        self.ff = Residual(FeedForward(d_model, d_ff=d_ff, dropout=ff_dropout))

    def forward(self, x, mask=None):
        out = self.attn(x, mask=mask)
        return self.ff(out)

# Cell
class TransformerEncoderNLN(Module):
    """Stack of TransformerEncoderBlocks"""
    def __init__(self,
                 d_model,
                 n_layers=6,
                 n_heads=8,
                 d_ff=None,
                 ff_dropout=0.1,
                 attn_dropout=0.1,
                 attn_bias=False,
                 causal=False,
                 prenorm=False,
                 shared_qk:bool=False,
                 final_norm=None):
        store_attr('d_model')
        self.layers = nn.ModuleList([])
        for _ in range(n_layers):
            self.layers.append(TransformerEncoderBlockNLN(d_model, n_heads, causal=causal,
                                    d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout,
                                    prenorm=prenorm, attn_bias=attn_bias, shared_qk=shared_qk))
        self.norm = None if final_norm is None else final_norm(d_model)

    def forward(self, x, mask=None):
        for layer in self.layers: x = layer(x, mask=mask)
        if self.norm is not None: x = self.norm(x)
        return x

# Cell
class TransformerLMNLN(Module, LMMixin):
    """
    tmp
    Basic Transformer for language modelling

    Parameters:
        * vocab_sz: int
        * d_model: int - inner dimension of the model
        * n_layers: int (default: 6)
        * n_heads: int (default: 8)
        * d_ff: int - inner dimension of the pointwise FeedForward net, if None defaults to 4*d_model
        * attn_dropout: float - attention dropout
        * ff_dropout: float - feed-forward dropout
        * emb_dropout: float - embedding dropout
        * causal: bool (default: True) - if True does causal masking automatically
        * max_seq_len: int (default: 512)
        * tie_weights: bool - if True target embedding weights are used for computation output projection
        * prenorm: bool - wether to use PreNorm or PostNorm
        * attn_bias: bool - wether to allow biases in attention projection layers
        * pad_idx: int - padding token id, required for autogeneration of padding mask
        * pos_enc: str from {'absolute', 'fixed', 'axial'} - type of positional encoding to use
        * axial_shape: tuple - [optional] should be factors of max_seq_len
        * axial_emb_dims: tuple - [optional] axial embedding components, should sum to d_model
    Inputs:
        * x - input ids, shape [bs, sl]
        * mask - optional boolean mask, shape [bs, sl]
    Returns:
        * logits - target token logits, shape [bs, sl, vocab_sz]
    """
    def __init__(self,
                 vocab_sz:int,
                 d_model:int,
                 n_layers:int=6,
                 n_heads:int=8,
                 d_ff:int=None,
                 attn_dropout:float=0.1,
                 ff_dropout:float=0.1,
                 emb_dropout:float=0.1,
                 tie_weights:bool=True,
                 causal:bool=True,
                 pos_enc:str='absolute',
                 max_seq_len:int=512,
                 axial_shape:tuple=None,
                 axial_emb_dims:tuple=None,
                 pad_idx:int=None,
                 prenorm:bool=False,
                 attn_bias:bool=False,
                 shared_qk:bool=False):
        store_attr()
        self.emb = TransformerEmbedding(vocab_sz, d_model, max_seq_len, dropout=emb_dropout,
                                        pos_enc=pos_enc, axial_shape=axial_shape,
                                        axial_emb_dims=axial_emb_dims)
        final_norm = None
        self.encoder = TransformerEncoderNLN(d_model, n_layers, n_heads, causal=causal, d_ff=d_ff,
                                          attn_dropout=attn_dropout, ff_dropout=ff_dropout,
                                          prenorm=prenorm, attn_bias=attn_bias,
                                          shared_qk=shared_qk, final_norm=final_norm)
        self.proj = nn.Linear(d_model, vocab_sz)
        if tie_weights: self.proj.weight = self.emb.emb.weight

    def forward(self, x, mask=None):
        x = self.emb(x)
        x = self.encoder(x, mask=mask)
        return self.proj(x)

# Cell
def fixup_init(model):
    "Applies FixUp initialization to LM (proto ver)"
    n_blocks = len(model.encoder.layers)*2
    for l in model.encoder.layers:
        l.attn.sublayer.in_proj.to_q.weight.data *= n_blocks**(-1/2)
        l.attn.sublayer.in_proj.to_kv.weight.data *= n_blocks**(-1/4)
        l.attn.sublayer.out_proj.weight.data *= 0.

        l.ff.sublayer.net[0].weight.data *= n_blocks**-0.5
        l.ff.sublayer.net[0].bias.data.zero_()
        l.ff.sublayer.net[3].weight.data.zero_()
        l.ff.sublayer.net[3].bias.data.zero_()

    model.proj.weight.data.zero_()
    model.proj.bias.data.zero_()

# Cell
class Scale(Module):
    def  __init__(self, scale=1.):
        self.scale = torch.nn.Parameter(torch.ones(1)*scale)
    def forward(self, x):
        return x * self.scale

class Shift(Module):
    def __init__(self):
        self.bias = torch.nn.Parameter(torch.zeros(1))
    def forward(self, x):
        return x + self.bias

class ShiftScale(Module):
    def __init__(self, sublayer, scale=1.):
        self.sublayer = sublayer
        self.shift = Shift()
        self.scale = Scale()
    def forward(self, x, **kwargs):
        x = self.shift(x)
        x = self.sublayer(x, **kwargs)
        return self.scale(x)


# Cell
class TransformerEncoderBlockNLN2(Module):
    """
    tmp
    Bacis transformer encoder block. Consists of multi-head attention and positional
    feedforward layers
    """
    def __init__(self,
                 d_model:int,
                 n_heads:int = 8,
                 d_ff:int = None,
                 attn_dropout:float = 0.1,
                 ff_dropout:float = 0.1,
                 causal:bool = False,
                 attn_bias:bool = False,
                 prenorm:bool=False,
                 shared_qk:bool=False):
        store_attr('attn_dropout') # mb separate argument attn_post_dropout
        self.attn = Residual(ShiftScale(Attention(d_model, n_heads=n_heads, causal=causal, dropout=attn_dropout, bias=attn_bias, shared_qk=shared_qk)))
        self.ff = Residual(FeedForwardFixup(d_model, d_ff=d_ff, dropout=ff_dropout))

    def forward(self, x, mask=None):
        out = self.attn(x, mask=mask)
        return self.ff(out)


# Cell
class TransformerEncoderNLN2(Module):
    """Stack of TransformerEncoderBlocks"""
    def __init__(self,
                 d_model,
                 n_layers=6,
                 n_heads=8,
                 d_ff=None,
                 ff_dropout=0.1,
                 attn_dropout=0.1,
                 attn_bias=False,
                 causal=False,
                 prenorm=False,
                 shared_qk:bool=False,
                 final_norm=None):
        store_attr('d_model')
        self.layers = nn.ModuleList([])
        for _ in range(n_layers):
            self.layers.append(TransformerEncoderBlockNLN2(d_model, n_heads, causal=causal,
                                    d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout,
                                    prenorm=prenorm, attn_bias=attn_bias, shared_qk=shared_qk))
        self.norm = None if final_norm is None else final_norm(d_model)

    def forward(self, x, mask=None):
        for layer in self.layers: x = layer(x, mask=mask)
        if self.norm is not None: x = self.norm(x)
        return x

# Cell
class TransformerLMNLN2(Module, LMMixin):
    """
    tmp
    Basic Transformer for language modelling

    Parameters:
        * vocab_sz: int
        * d_model: int - inner dimension of the model
        * n_layers: int (default: 6)
        * n_heads: int (default: 8)
        * d_ff: int - inner dimension of the pointwise FeedForward net, if None defaults to 4*d_model
        * attn_dropout: float - attention dropout
        * ff_dropout: float - feed-forward dropout
        * emb_dropout: float - embedding dropout
        * causal: bool (default: True) - if True does causal masking automatically
        * max_seq_len: int (default: 512)
        * tie_weights: bool - if True target embedding weights are used for computation output projection
        * prenorm: bool - wether to use PreNorm or PostNorm
        * attn_bias: bool - wether to allow biases in attention projection layers
        * pad_idx: int - padding token id, required for autogeneration of padding mask
        * pos_enc: str from {'absolute', 'fixed', 'axial'} - type of positional encoding to use
        * axial_shape: tuple - [optional] should be factors of max_seq_len
        * axial_emb_dims: tuple - [optional] axial embedding components, should sum to d_model
    Inputs:
        * x - input ids, shape [bs, sl]
        * mask - optional boolean mask, shape [bs, sl]
    Returns:
        * logits - target token logits, shape [bs, sl, vocab_sz]
    """
    def __init__(self,
                 vocab_sz:int,
                 d_model:int,
                 n_layers:int=6,
                 n_heads:int=8,
                 d_ff:int=None,
                 attn_dropout:float=0.1,
                 ff_dropout:float=0.1,
                 emb_dropout:float=0.1,
                 tie_weights:bool=True,
                 causal:bool=True,
                 pos_enc:str='absolute',
                 max_seq_len:int=512,
                 axial_shape:tuple=None,
                 axial_emb_dims:tuple=None,
                 pad_idx:int=None,
                 prenorm:bool=False,
                 attn_bias:bool=False,
                 shared_qk:bool=False):
        store_attr()
        self.emb = TransformerEmbedding(vocab_sz, d_model, max_seq_len, dropout=emb_dropout,
                                        pos_enc=pos_enc, axial_shape=axial_shape,
                                        axial_emb_dims=axial_emb_dims)
        final_norm = None
        self.encoder = TransformerEncoderNLN2(d_model, n_layers, n_heads, causal=causal, d_ff=d_ff,
                                          attn_dropout=attn_dropout, ff_dropout=ff_dropout,
                                          prenorm=prenorm, attn_bias=attn_bias,
                                          shared_qk=shared_qk, final_norm=final_norm)
        self.proj = nn.Linear(d_model, vocab_sz)
        if tie_weights: self.proj.weight = self.emb.emb.weight

    def forward(self, x, mask=None):
        x = self.emb(x)
        x = self.encoder(x, mask=mask)
        return self.proj(x)

# Cell
def fixup_init2(model):
    "Applies FixUp initialization to LM (proto ver)"
    n_blocks = len(model.encoder.layers)*2
    for l in model.encoder.layers:
        #?? is -1/6 right or should be -1/2;-1/4
        l.attn.sublayer.sublayer.in_proj.to_q.weight.data *= n_blocks**(-1/6)
        l.attn.sublayer.sublayer.in_proj.to_kv.weight.data *= n_blocks**(-1/6)
        l.attn.sublayer.sublayer.out_proj.weight.data *= 0.

        l.ff.sublayer.net.fc1.weight.data *= n_blocks**-0.5
        l.ff.sublayer.net.fc1.bias.data.zero_()
        l.ff.sublayer.net.fc2.weight.data.zero_()
        l.ff.sublayer.net.fc2.bias.data.zero_()

    model.proj.weight.data.zero_()
    model.proj.bias.data.zero_()

# Cell
class AdminResidual(Module):
    def __init__(self, sublayer, d_model):
        self.sublayer = sublayer
        self.w = torch.nn.Parameter(torch.ones(d_model))
    def forward(self, x, *args, **kwargs):
        return x*self.w + self.sublayer(x, *args, **kwargs)


# Cell
class TransformerEncoderBlockAdmin(Module):
    """
    Bacis transformer encoder block. Consists of multi-head attention and positional
    feedforward layers
    """
    def __init__(self,
                 d_model:int,
                 n_heads:int = 8,
                 d_ff:int = None,
                 attn_dropout:float = 0.1,
                 ff_dropout:float = 0.1,
                 causal:bool = False,
                 attn_bias:bool = False,
                 prenorm:bool=False,
                 shared_qk:bool=False):
        store_attr('attn_dropout') # mb separate argument attn_post_dropout

        self.attn = PostNorm(d_model, AdminResidual(Attention(d_model, n_heads=n_heads, causal=causal, dropout=attn_dropout, bias=attn_bias, shared_qk=shared_qk), d_model))
        self.ff = PostNorm(d_model, AdminResidual(FeedForward(d_model, d_ff=d_ff, dropout=ff_dropout), d_model))

    def forward(self, x, mask=None):
        out = self.attn(x, mask=mask)
        return self.ff(out)

# Cell
class TransformerEncoderAdmin(Module):
    """Stack of TransformerEncoderBlocks"""
    def __init__(self,
                 d_model,
                 n_layers=6,
                 n_heads=8,
                 d_ff=None,
                 ff_dropout=0.1,
                 attn_dropout=0.1,
                 attn_bias=False,
                 causal=False,
                 prenorm=False,
                 shared_qk:bool=False,
                 final_norm=None):
        store_attr('d_model')
        self.layers = nn.ModuleList([])
        for _ in range(n_layers):
            self.layers.append(TransformerEncoderBlockAdmin(d_model, n_heads, causal=causal,
                                    d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout,
                                    prenorm=prenorm, attn_bias=attn_bias, shared_qk=shared_qk))
        self.norm = None if final_norm is None else final_norm(d_model)

    def forward(self, x, mask=None):
        for layer in self.layers: x = layer(x, mask=mask)
        if self.norm is not None: x = self.norm(x)
        return x

# Cell
class TransformerLMAdmin(Module, LMMixin):
    """
    tmp
    Basic Transformer for language modelling

    Parameters:
        * vocab_sz: int
        * d_model: int - inner dimension of the model
        * n_layers: int (default: 6)
        * n_heads: int (default: 8)
        * d_ff: int - inner dimension of the pointwise FeedForward net, if None defaults to 4*d_model
        * attn_dropout: float - attention dropout
        * ff_dropout: float - feed-forward dropout
        * emb_dropout: float - embedding dropout
        * causal: bool (default: True) - if True does causal masking automatically
        * max_seq_len: int (default: 512)
        * tie_weights: bool - if True target embedding weights are used for computation output projection
        * prenorm: bool - wether to use PreNorm or PostNorm
        * attn_bias: bool - wether to allow biases in attention projection layers
        * pad_idx: int - padding token id, required for autogeneration of padding mask
        * pos_enc: str from {'absolute', 'fixed', 'axial'} - type of positional encoding to use
        * axial_shape: tuple - [optional] should be factors of max_seq_len
        * axial_emb_dims: tuple - [optional] axial embedding components, should sum to d_model
    Inputs:
        * x - input ids, shape [bs, sl]
        * mask - optional boolean mask, shape [bs, sl]
    Returns:
        * logits - target token logits, shape [bs, sl, vocab_sz]
    """
    def __init__(self,
                 vocab_sz:int,
                 d_model:int,
                 n_layers:int=6,
                 n_heads:int=8,
                 d_ff:int=None,
                 attn_dropout:float=0.1,
                 ff_dropout:float=0.1,
                 emb_dropout:float=0.1,
                 tie_weights:bool=True,
                 causal:bool=True,
                 pos_enc:str='absolute',
                 max_seq_len:int=512,
                 axial_shape:tuple=None,
                 axial_emb_dims:tuple=None,
                 pad_idx:int=None,
                 prenorm:bool=False,
                 attn_bias:bool=False,
                 shared_qk:bool=False):
        store_attr()
        self.emb = TransformerEmbedding(vocab_sz, d_model, max_seq_len, dropout=emb_dropout,
                                        pos_enc=pos_enc, axial_shape=axial_shape,
                                        axial_emb_dims=axial_emb_dims)
        final_norm = None
        self.encoder = TransformerEncoderAdmin(d_model, n_layers, n_heads, causal=causal, d_ff=d_ff,
                                          attn_dropout=attn_dropout, ff_dropout=ff_dropout,
                                          prenorm=prenorm, attn_bias=attn_bias,
                                          shared_qk=shared_qk, final_norm=final_norm)
        self.proj = nn.Linear(d_model, vocab_sz)
        if tie_weights: self.proj.weight = self.emb.emb.weight

    def forward(self, x, mask=None):
        x = self.emb(x)
        x = self.encoder(x, mask=mask)
        return self.proj(x)


# Cell
class BreakFitCallback(Callback):
    order=-1
    "Cancels fit after one batch before weight update"
    def before_step(self):
        self.model.zero_grad(set_to_none=True)
        raise CancelStepException
    def after_step(self):
        raise CancelBatchException
    def after_batch(self):
        print('Fit canceled')
        raise CancelFitException

# Cell
def res_submodules(model):
    return [m.sublayer for m in learn.model.modules() if isinstance(m, AdminResidual)]

def res_modules(model):
    return [m for m in learn.model.modules() if isinstance(m, AdminResidual)]

# Cell
def variances(learn):
    return np.array([stat['std']**2 for stat in learn.activation_stats.stats[0]])

# variances(learn)

# Cell
def _init_scales(vars):
    return np.sqrt(np.cumsum(vars))
# scales = _init_scales(variances(learn))
# scales

# Cell
def admin_init(model, scales):
    ms = res_modules(model)
    for m, s in zip(ms, scales):
        m.w.data *= s