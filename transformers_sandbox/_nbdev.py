# AUTOGENERATED BY NBDEV! DO NOT EDIT!

__all__ = ["index", "modules", "custom_doc_links", "git_url"]

index = {"exists": "00_core.ipynb",
         "default": "00_core.ipynb",
         "expand_dim1": "00_core.ipynb",
         "max_neg_value": "00_core.ipynb",
         "setattr_on": "00_core.ipynb",
         "top_p_filter": "00_core.ipynb",
         "top_k_filter": "00_core.ipynb",
         "cache_method_decorator": "00_core.ipynb",
         "look_one_back": "00_core.ipynb",
         "chunked_sum": "00_core.ipynb",
         "sort_key_val": "00_core.ipynb",
         "batched_index_select": "00_core.ipynb",
         "do_cuda_timing": "00_core.ipynb",
         "model_performance": "00_core.ipynb",
         "total_params": "00_core.ipynb",
         "ModelParams": "00_core.ipynb",
         "CombineInputOutputCallback": "00_core.ipynb",
         "RemoveEOSCallback": "00_core.ipynb",
         "LossTargetShiftCallback": "00_core.ipynb",
         "PadBatchCallback": "00_core.ipynb",
         "AddEOSID": "00_core.ipynb",
         "LabelSmoothingCrossEntropy": "00_core.ipynb",
         "LabelSmoothingCrossEntropyFlat": "00_core.ipynb",
         "Learner.distrib_ctx": "00_core.ipynb",
         "Residual": "01_layers.ipynb",
         "PostNorm": "01_layers.ipynb",
         "PreNorm": "01_layers.ipynb",
         "FeedForward": "01_layers.ipynb",
         "get_axial_shape": "01_layers.ipynb",
         "get_axial_dims": "01_layers.ipynb",
         "AbsolutePositionalEmbedding": "01_layers.ipynb",
         "FixedPositionalEmbedding": "01_layers.ipynb",
         "TransformerEmbedding": "01_layers.ipynb",
         "MASK_VAL": "02_attention.ipynb",
         "SELF_ATTN_MASK_VAL": "02_attention.ipynb",
         "AttnInProj": "02_attention.ipynb",
         "AttnInProjV2": "02_attention.ipynb",
         "SharedQKAttnInProj": "02_attention.ipynb",
         "ScaledDotProdAttention": "02_attention.ipynb",
         "Attention": "02_attention.ipynb",
         "MemEfficientAttention": "02_attention.ipynb",
         "ChunkedDotProdAttention": "02_attention.ipynb",
         "ChunkedAttention": "02_attention.ipynb",
         "AdditiveInProj": "02_attention.ipynb",
         "AdditiveAttention": "02_attention.ipynb",
         "LSHAttention": "02_attention.ipynb",
         "LSHSelfAttention": "02_attention.ipynb",
         "ReformerAttention": "02_attention.ipynb",
         "LMMixin": "03_models.transformer.ipynb",
         "EncDecMixin": "03_models.transformer.ipynb",
         "TransformerEncoderBlock": "03_models.transformer.ipynb",
         "TransformerEncoder": "03_models.transformer.ipynb",
         "TransformerDecoderBlock": "03_models.transformer.ipynb",
         "TransformerDecoderBlockV2": "03_models.transformer.ipynb",
         "TransformerDecoder": "03_models.transformer.ipynb",
         "TransformerLM": "03_models.transformer.ipynb",
         "transformer_lm_splits": "03_models.transformer.ipynb",
         "Transformer": "03_models.transformer.ipynb",
         "transformer_splits": "03_models.transformer.ipynb",
         "LowMemEncoderBlock": "03_models.transformer.ipynb",
         "LowMemEncoder": "03_models.transformer.ipynb",
         "ChunkedTransformerLM": "03_models.transformer.ipynb",
         "Chunk": "04a_models.reformer.ipynb",
         "ChunkedFeedForward": "04a_models.reformer.ipynb",
         "Deterministic": "04a_models.reformer.ipynb",
         "ReversibleBlock": "04a_models.reformer.ipynb",
         "IrreversibleBlock": "04a_models.reformer.ipynb",
         "ReversibleSequence": "04a_models.reformer.ipynb",
         "RevSwap": "04a_models.reformer.ipynb",
         "RevHalfResidual": "04a_models.reformer.ipynb",
         "RevChunk": "04a_models.reformer.ipynb",
         "RevMerge": "04a_models.reformer.ipynb",
         "ReversibleSequenceV2": "04a_models.reformer.ipynb",
         "ReversibleEncoder": "04a_models.reformer.ipynb",
         "ReversibleDecoder": "04a_models.reformer.ipynb",
         "ReversibleLM": "04a_models.reformer.ipynb",
         "ReversibleTransformer": "04a_models.reformer.ipynb",
         "ReversibleEncoderV2": "04a_models.reformer.ipynb",
         "ReversibleLMV2": "04a_models.reformer.ipynb",
         "LSHEncoderBlock": "04a_models.reformer.ipynb",
         "LSHEncoder": "04a_models.reformer.ipynb",
         "LSHLM": "04a_models.reformer.ipynb",
         "ReformerEncoder": "04a_models.reformer.ipynb",
         "ReformerLM": "04a_models.reformer.ipynb",
         "reformer_lm_splits": "04a_models.reformer.ipynb",
         "wrap_sublayer": "04x_models.xtransformer.ipynb",
         "XEncoderBlock": "04x_models.xtransformer.ipynb",
         "XEncoder": "04x_models.xtransformer.ipynb",
         "XDecoderBlock": "04x_models.xtransformer.ipynb",
         "XDecoderBlockV2": "04x_models.xtransformer.ipynb",
         "XDecoder": "04x_models.xtransformer.ipynb",
         "XTransformerLM": "04x_models.xtransformer.ipynb",
         "XTransformer": "04x_models.xtransformer.ipynb",
         "XConfig": "04x_models.xtransformer.ipynb",
         "ByteTextTokenizer": "05_tokenizers.ipynb",
         "SubwordTextEncoder": "05_tokenizers.ipynb",
         "read_lines": "06_data.ipynb",
         "convert_data_to_seq_length": "06_data.ipynb",
         "read_and_prepare_data": "06_data.ipynb",
         "TwinSequence": "06_data.ipynb",
         "MaskTargCallback": "06_data.ipynb",
         "DeterministicTwinSequence": "06_data.ipynb",
         "MaskedAccuracy": "07_metrics.ipynb",
         "BPC": "07_metrics.ipynb",
         "bpc": "07_metrics.ipynb",
         "Adafactor": "08_optimizers.ipynb",
         "adafactor": "08_optimizers.ipynb",
         "Learner.gather_args": "09_tracking.ipynb",
         "update_sig": "10_config.ipynb",
         "ConfigBase": "10_config.ipynb",
         "SyntheticConfig": "10_config.ipynb",
         "TransformerLMConfigEnwik8": "10_config.ipynb",
         "ReversibleLMConfigEnwik8": "10_config.ipynb",
         "NHashesConfig": "10_config.ipynb",
         "NLayersConfig": "10_config.ipynb",
         "ReversibleTransformerConfigWMT": "10_config.ipynb",
         "TransformerConfigWMT": "10_config.ipynb"}

modules = ["core.py",
           "layers.py",
           "attention.py",
           "transformer.py",
           "reformer.py",
           "xtransformer.py",
           "tokenizers.py",
           "data.py",
           "metrics.py",
           "optimizers.py",
           "tracking.py",
           "config.py"]

doc_url = "https://aikindergarten.github.io/transformers_sandbox/"

git_url = "https://github.com/aikindergarten/transformers_sandbox/tree/master/"

def custom_doc_links(name): return None
